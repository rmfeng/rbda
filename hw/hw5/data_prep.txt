# This text files details the processing and EDA of the weather dataset

The files from the weather dataset comes in several thousand zipped files, one from each year for each weather station. As a result, we needed to unzip all of the files and concatenate them into 1 large file. 

The folder structure looks as follows:
./data
    /1901
        /[weatherid]_[year].gz
        /[weatherid]_[year].gz
        /...
    /...
    /2019/
        /[weatherid]_[year].gz
        /...

# unzipping all files linux command:
gunzip -r ./

# combining all text files and appending filename to each line
find -type f -exec awk '{print $0 " " FILENAME}' {} + > ../combined.txt

# creating a subset file with which to prototype code
head -10000 combined.txt > subset.txt

Each line of the weather data looks like this:
2019 06 29 21 227 65 10180 310 15 0 -9999 -9999 ./720997-99999-2019

with each fixed length position containing a certain field in this order:
year, month, day, hour, temp, humidity, pressure, wind direction, wind speed, sky condition, precip_1hr, precip_6hr, station_id+year
any null values are denoted by -9999

Also, we have a weather station definitions file called station_ids.txt that looks like this:

USAF   WBAN  STATION NAME                  CTRY ST CALL  LAT     LON      ELEV(M) BEGIN    END
720997 99999 WXPOD 7018                                  +00.000 +000.000 +7018.0 20110309 20130730
...

We then use global connect to upload the files to dumbo and then to hdfs

# putting files into hdfs
hfs -put combined.txt
hfs -put subset.txt
hfs -put station_ids.txt

# output
[rf1316@login-1-1 weather]$ hfs -ls
Found 7 items
drwx------+  - rf1316 users            0 2019-07-18 20:00 .Trash
drwxr-xr-x+  - rf1316 users            0 2019-05-12 21:36 .sparkStaging
drwx------+  - rf1316 users            0 2019-07-17 19:34 .staging
-rwxr-xr-x+  3 rf1316 users 176290860198 2019-07-13 21:58 combined.txt
-rwxr-xr-x+  3 rf1316 users      2978095 2019-07-14 14:12 station_ids.txt
-rw-r--r--+  3 rf1316 users       820000 2019-07-17 19:19 subset.txt


To explore the data, we write some map reduce jobs to do the EDA, 

# running MR commands
hjs -D mapred.reduce.tasks=1 -file src/ -mapper src/edc_mapper.sh -reducer src/edc_reducer.sh -input subset.txt -output avg_temp



